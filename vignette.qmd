---
bibliography: references.bib
---

# Sentiment Analysis

## Introduction

### Four Sentiment Lexicon Libraries

Sentiment Lexicons are used to label a single word with a positive, negative, or neutral connotation. The way that the four datasets approach this classification is different.

#### [AFINN](https://github.com/fnielsen/afinn)

Assigns one single word with a numeric value between -5 and 5. Values in the negative range are words with a negative connotation. Those in the positive range are words with a positive connotation. The numeric values also have a degree, for example, positive ranges from 1 to 5, 5 being most positive and 1 being least positive.

```{r}
library(textdata)
library(tidytext)
get_sentiments("afinn")
```

#### [Bing](https://emilhvitfeldt.github.io/textdata/reference/lexicon_bing.html)

Assigns "positive" or "negative". The most simple of the sentiment lexicon libraries.

```{r}
get_sentiments("bing")
```

#### [NRC](http://saifmohammad.com/WebPages/lexicons.html)

Similar to bing in that it assigns one single word into a category but instead of just sentiments "negative" and "positive", there is also an emotions related category that has "anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", and "trust".

```{r}
get_sentiments("nrc")
```

#### [Loughran](https://emilhvitfeldt.github.io/textdata/reference/lexicon_loughran.html)

Used primarily for financial terms. It takes into account financial words that may appear to be a certain sentiment in other libraries but in a financial context, may have a different meaning.

```{r}
get_sentiments("loughran")
```

### Objective

In this vignette, we will be exploring the IMDB-raw dataset file. This dataset comprises of two columns, "review" and "sentiment". The "review" column is made up of reviews left on movies on the IMDB platform in the form of text data. The "sentiment" column is a binary score "positive" or "negative" that were labeled by hand. If a review is labeled as "negative" then it what is known as a "bad review" or a sub-par experience. If a review is labeled as "positive" then the reviewer had an enjoyable experience.

For this experiment, we will be breaking apart the reviews into single words. From here, the words are associated with one score from each of the four libraries (AFINN, Bing, NRC, Loughran). Taking the most common sentiment, or, in the case of AFINN - the average value, we will obtain four overall average "sentiments" that summarize the reviews. One other predictor added is the length of the reviews as this may have had an impact in the hand-picked scores. In a sense, we are attempting to simulate the hand-picked sentiments.

The four sentiments found will be used in a logistic model to predict how accurately they were at labeling the reviews. The predictions will be compared to the hand-picked values.

### Exploratory Data Analysis

## Logistic Regression Model

### Load Packages and Data

```{r}
library(tidyverse)
library(tidytext)
library(tidymodels)
library(tokenizers)
library(textstem)
library(stopwords)
library(textdata)
```

```{r}
# loading in the dataset.
dataset <- read.csv("/Users/shannon/Documents/PSTAT197/vignette-sentiment-analysis")
```

### Creation of Predcitors

```{r}
# matches words in reviews with words in afinn library
# provides an afinn score for each individual word
dataset_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  inner_join(get_sentiments("afinn"))

# calculates the mean of afinn score by id
afinn_scores <- dataset_tokens %>%
  group_by(.id) %>%
  summarize(afinn_score = mean(value))

# merge with main dataset
dataset_clean <- merge(dataset_clean, afinn_scores, by = ".id")
```

```{r}
# Uses the Bing library to categorize words into positive and negative categories
# Our Bing score is based on whether there are more positive or negative words in each observation
bing_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  inner_join(get_sentiments("bing"))

# Calculating Bing Sentiment
# Joining by id and deciding sentiment based on the higher amount of positive or negative scores
bing_scores <- bing_tokens %>%
  group_by(.id) %>%
  summarize(bing_score = max(sentiment))

# Merge with main dataset
dataset_clean <- merge(dataset_clean, bing_scores, by = ".id")

```

```{r}
### Predicting Sentiment with Loughran ###
dataset_loughran_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  inner_join(get_sentiments("loughran"))

loughran_scores <- dataset_loughran_tokens %>%
  group_by(.id) %>%
  summarize(loughran_score = max(sentiment))

dataset_clean <- merge(dataset_clean, loughran_scores, by = ".id")

```

```{r}
### Predicting sentiment with NRC ###

dataset_nrc_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  inner_join(get_sentiments("nrc"))

nrc_scores <- dataset_nrc_tokens %>%
  group_by(.id) %>%
  summarize(nrc_score = max(sentiment))

dataset_clean <- merge(dataset_clean, nrc_scores, by = ".id")

```

## Findings
