```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Sentiment Analysis

#### By Vardan Martirosyan, Raymond Lee, Piero Trujillo, Shannon Rumsey, Yutong Wang

## Introduction

### Sentiment Analysis and Lexicons

Sentiment analysis (also called opinion mining) is the process of analyzing text to understand its attitude. This can be helpful when combing through large amounts of text data as oppose to doing to by hand. One of the most common ways to create a sentiment label for long pieces of text is to consider it as the sum of individual words, each with its own sentiment label. This is similar to how humans use their understanding of emotional intent to infer whether a text is positive or negative; each word is read for its value, and at the end, they are combined together to create something of a larger meaning.

There are many different lexicon libraries that are used to evaluate text meaning and label them, typically in the form of single-words with an associated rating. Please note that lexicon libraries are not limited to English and can be used for many different languages. For the purposes of this vignette, we will be working with only English words and utilize four lexicons: AFINN, Bing, NRC, and Loughran.

#### AFINN

Of the four sentiment libraries we will be using, AFINN is the only numeric one. This library assigns each word with a numeric value between -5 and 5. Values in the negative range are words with a negative connotation while those in the positive range are words with a positive connotation. The numeric values also have act as degrees, for example, positive ranges from 1 to 5, 5 being most positive and 1 being least positive.

```{r}
library(textdata)
library(tidytext)
get_sentiments("afinn")
```

#### Bing

Bing is one of the more common lexicon libraries to use. It assigns "positive" or "negative", the most simple of the sentiment lexicon libraries.

```{r}
bing <- get_sentiments("bing")
bing
```

#### NRC

This one is similar to Bing in that it assigns one single word into a category, but instead of just sentiments "negative" and "positive", there are also emotions related category that has "anger", "anticipation", "disgust", "fear", "joy", "sadness", "surprise", and "trust".

```{r}
nrc <- get_sentiments("nrc")
nrc
```

#### Loughran

Loughran is used primarily for financial terms. It takes into account financial words that may appear to be a certain sentiment in other libraries but may have a different meaning in a financial context.

```{r}
loughran<-get_sentiments("loughran")
loughran
```

### Objective

In this vignette, we will be exploring the IMDB-raw dataset file. This dataset comprises of two columns, "review" and "sentiment". The "review" column is made up of reviews left on the IMDB platform in the form of text data. The "sentiment" column is a binary score, "positive" or "negative", that was labeled by hand.

For this vignette, we will be breaking apart the reviews into single words. From here, the words are associated with one score from each of the four libraries (AFINN, Bing, NRC, Loughran). Taking the most common sentiment, or, in the case of AFINN - the average value, we will obtain four overall average "sentiments" that summarize the reviews. One other predictor added is the length of the reviews as this may have had an impact in the hand-picked scores. In a sense, we are attempting to simulate the hand-picked sentiments.

The four sentiments found will be used in a logistic model to predict how accurately they were at labeling the reviews. The predictions will be compared to the hand-picked values.

### Load Packages and Data

```{r}
library(tidyverse)
library(tidytext)
library(tidymodels)
library(tokenizers)
library(textstem)
library(stopwords)
library(textdata)
library(dplyr)
library(corrplot)
```

We can then load in the dataset as follows:

```{r}
# loading in the dataset
# make sure to set working directory to folder containing vignette
dataset <- read.csv("data/IMDB-raw.csv")
```

### Cleaning the Dataset

We want to clean the dataset so that we can build a model with it. To do this, we will define and use two functions.

```{r}
### Functions for Text Cleaning ###

# Creating several wrapper functions to clean up the data.

# This first function removes the "<br />" terms from the text, as these are not words. It also sets everything to lower case so that we have a unified format to work with. 
clean_fn <- function(.text){
  str_replace_all(.text, "<br />", "") %>% tolower()
}

# This function removes all punctuation. 
clean_fn2 <- function(.text){
  str_remove_all(.text, '[[:punct:]]') %>%
    tolower()
}
```

Having defined these functions, we can now clean the dataset:

```{r}
# Applying this function to all of the reviews in our dataset.
dataset_clean <- dataset %>%
  mutate(review = clean_fn(review)) %>%
  mutate(review = clean_fn2(review))
```

Finally, we can add an 'id' column to our dataset and write the cleaned dataset to our 'data' folder for easy use later on.

```{r}
# Adding a column with id's for each observation.
dataset_clean <- tibble::rowid_to_column(dataset_clean, ".id")

#writing the data.
write.csv(dataset_clean, file = "data/IMDB-clean.csv")
```

### Creation of Predictors

In this section, we will add five predictors to the dataset, one predictor being the length of the review and the other four predictors being the sentiments of each review.

First, create the length predictor.

```{r}
#Adding a column with the lengths of each review for each observation.
dataset_clean <- dataset_clean %>%
  mutate(length = nchar(dataset$review, type = "chars", allowNA = FALSE, keepNA = NA))

```

Then, compute the 'AFINN' sentiment for each review and merge it to the dataset as follows.

```{r}
# matches words in reviews with words in afinn library
# provides an afinn score for each individual word
afinn_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  full_join(get_sentiments("afinn"))

#Assign a score of 0 for the words that are not included by the lexicon
afinn_tokens$value[is.na(afinn_tokens$value)] <- 0

# calculates the mean of afinn score by id
afinn_scores <- afinn_tokens %>%
  group_by(.id) %>% 
  summarize(afinn_score = mean(value[value!=0]))

#Assign a score of 0 for the reviews that contains no word that are included by the lexicon
afinn_scores$afinn_score[is.na(afinn_scores$afinn_score)] <- 0

# merge with main dataset
dataset_clean <- merge(dataset_clean, afinn_scores, by = ".id")
```

Next, we will add the 'bing' sentiment.

```{r}
# Uses the Bing library to categorize words into positive and negative categories
# Our Bing score is based on whether there are more positive or negative words in each observation
bing_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  full_join(get_sentiments("bing"))

#Assign neutral for all the words that are not a part of the lexicon
`%notin%` <- Negate(`%in%`)
bing_tokens$sentiment[bing_tokens$word %notin% bing$word] <- "neutral"

# Calculating Bing Sentiment
# Joining by id and deciding sentiment based on the higher amount of positive or negative scores
bing_scores <- bing_tokens %>%
  group_by(.id) %>%
  summarize(bing_score = max(sentiment[sentiment!="neutral"]))

#Assign neutral for the reviews that includes no words that are in the lexicon
bing_scores$bing_score[is.na(bing_scores$bing_score)] <- "neutral"

# Merge with main dataset
dataset_clean <- merge(dataset_clean, bing_scores, by = ".id")

```

Continuing onward, we will compute the 'Loughran' sentiment for each review and put it into the dataset as follows.

```{r}
### Predicting Sentiment with Loughran
loughran_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  full_join(get_sentiments("loughran"))

#Assign neutral for all the words that are not a part of the lexicon
loughran_tokens$sentiment[loughran_tokens$word %notin% loughran$word] <- "neutral"
# Joining by id and deciding sentiment based on the higher amount of positive or negative scores
loughran_scores <- loughran_tokens %>%
  group_by(.id) %>%
  summarize(loughran_score = max(sentiment[sentiment!="neutral"]))

#Assign neutral for the reviews that includes no words that are in the lexicon
loughran_scores$loughran_score[is.na(loughran_scores$loughran_score)] <- "neutral"

dataset_clean <- merge(dataset_clean, loughran_scores, by = ".id")

```

Finally, we will add the 'NRC' sentiment.

```{r}
### Predicting sentiment with NRC
nrc_tokens <- dataset_clean %>% 
  unnest_tokens(word, review) %>%
  anti_join(get_stopwords()) %>%
  full_join(get_sentiments("nrc"))

#Assign neutral for all the words that are not a part of the lexicon
nrc_tokens$sentiment[nrc_tokens$word %notin% nrc$word] <- "neutral"

# Joining by id and deciding sentiment based on the higher amount of positive or negative scores
nrc_scores <- nrc_tokens %>%
  group_by(.id) %>%
  summarize(nrc_score = max(sentiment))

#Assign neutral for the reviews that includes no words that are in the lexicon
nrc_scores$nrc_score[is.na(nrc_scores$nrc_score)] <- "neutral"

dataset_clean <- merge(dataset_clean, nrc_scores, by = ".id")

```

This concludes the construction of our cleaned dataset, as we now have all of the predictors that we want.

### Exploratory Data Analysis

In this section, we will explore our IMDB Review dataset with a plethora of graphs and other visualizations.

##### Corrplot

```{r}
library('corrplot')
length <- dataset_clean$length
afinn_score <- dataset_clean$afinn_score
corrplot(cor(data.frame(length, afinn_score)), type = "lower", method = "shade")
```

In this correlation plot, we see how the length of each review affects its AFINN score. Although our variables have a slight negative correlation with each other, review length does not seem to strongly affect AFINN score.

##### Distribution of Sentiment Scores Among Reviews

We will use barplots to show how the distribution of positive and negative scores between different sentiment analysis models varies. We will plot the distributions of our Bing and Loughran scores against the actual split of the data to see how well we were able to classify our movie reviews.

To do so, we must first count the amount of positive and negative scores for each sentiment analysis model. We can do this by grouping each dataframe by their sentiment score column and then counting each group.

```{r}
score_table = dataset_clean %>% group_by('sentiment') %>% count(sentiment, sort=TRUE)

bing_table = dataset_clean %>% group_by('bing_score') %>% count(sentiment, sort=TRUE)

loughran_table = dataset_clean %>% group_by('loughran_score') %>% count(sentiment, sort=TRUE)
```

##### Bar Plot of Actual Distribution

We can now use a faceted bar plot to compare the distributions in our models versus the real distribution.

```{r}
real_bar <- ggplot(data=score_table, aes(x=sentiment, y=n)) +
  geom_bar(stat='identity', color="blue", fill="white", width=0.5) + ggtitle("Actual Data Split")
```

##### Bing Distribution Bar Plot

```{r}
bing_bar <- ggplot(data=bing_table, aes(x=sentiment, y=n)) +
  geom_bar(stat='identity', color="orange", fill="white", width=0.5) + ggtitle("Bing Data Split")
```

##### Loughran Distribution Bar Plot

```{r}
loughran_bar <- ggplot(data=loughran_table, aes(x=sentiment, y=n)) +
  geom_bar(stat='identity', color="green", fill="white", width=0.5) + ggtitle("Loughran Data Split")
```

##### Facet All Plots Together

```{r}
library(ggpubr)
figure <- ggarrange(real_bar, bing_bar, loughran_bar,
                    ncol = 3, nrow = 1)
figure
```

Looking at our bar plots, we can see that the real split between our sentiment score values is surprisingly 50.07% positive and 49.99% negative. At first, this split seemed too good to be true but after checking with the publisher of the original dataset we confirmed that the data is in fact split about evenly. This graph also shows us that our NRC and Bing sentiment analysis models were able to classify reviews with perfect accuracy. You wouldn't be able to tell the differences in accuracy unless you looked at their real values in their accuracy tables which show a perfect classification rate.

##### Distribution of Number of Words Per Review

In this graph, we explore how the length of movie reviews is distributed in our dataset. We counted every movie review's length by adjusting our "stat" variable to count.

```{r}
word_count_bar <- ggplot(data=dataset_clean, aes(x=length)) +
  geom_bar(stat='count', color="purple", fill="white", width=2) + ggtitle("Distribution of Number of Words Per Review") 

word_count_bar
```

Our graph shows us that the majority of our movie reviews have an average word length under 1500.

##### Most Common Words In a Sentiment Analysis Model

We look at our NRC sentiment analysis dataframe to find the most common words in our movie reviews that are also found in our NRC library.

To do so, we must first count every time a word appeared in a movie review for each sentiment analysis model.

```{r}
word_counts <- nrc_tokens %>% count(word)
word_counts <- word_counts[order(-word_counts$n),]
```

Now we will subset our dataset to only use the top 20 most popular words.

```{r}
first_20 <- word_counts[1:20,]

first_20_bar <- ggplot(data=first_20, aes(x=reorder(word,-n), y=n)) +
  geom_bar(stat='identity', color="purple", fill="white", width=0.5) + ggtitle("Most Popular Words in Movie Reviews (NRC Sentiment Analysis)") 

first_20_bar
```

Our plot shows us that "good" and "bad" are by far the most common words found in our movie reviews that were recognized by the NRC library. These two words are also likely to be best indicators of whether a review is positive or negative.

##### Tree Map of Most Popular Movie Review Words

We can also use the 'treemap' package to visualize word popularity by size.

```{r}
library(treemap)

treemap(first_20, # dataframe
        index=("word"),  # categorical column
        vSize = "n",  # numerical column
        type="index", # Type sets the organization and color scheme of your treemap
        palette = "Greens",  # color palette from RColorBrewer preset. You can also make a vector of your own color values.
        title="Most Popular Movie Review Words", # title
        fontsize.title = 14 # font size
)
```

##### Word Clouds

Using each sentiment analysis dataframe, we can count every instance of a word and create word clouds of the most popular words recognized from every model's corresponding library.

##### Bing Word Cloud

```{r}
library(wordcloud)

bing_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

##### NRC Word Cloud

```{r}
nrc_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

##### Loughran Word Cloud

```{r}
loughran_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

```

##### AFINN Word Cloud

```{r}
afinn_tokens %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

We can also classify word clouds into positive and negative sentiments by sorting every word by their sentiment value and using acast() which casts our dataframe into an array of count values "n". We then call comparison.cloud() to make a word cloud after comparing sentiment score frequencies. After, we can apply our favorite hexadecimal color values to represent positive and negative words.

##### Bing Negative & Positive Word Cloud

```{r}
library(reshape2)

bing_scores %>%
  inner_join(bing_tokens) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#FF6F61", "#34568B"),
                   max.words = 100)
```

##### AFINN Negative & Positive Word Cloud

```{r}
afinn_scores %>%
  inner_join(afinn_tokens) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#9B2335", "#55B4B0"),
                   max.words = 100)
```

##### Loughran Negative & Positive Word Cloud

```{r}
loughran_scores %>%
  inner_join(loughran_tokens) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("#E15D44", "#7FCDCD"),
                   max.words = 100)
```

### Building and Testing the Logistic Regression Model

Having constructed our dataset, we can now build the logistic regression model, and test it's performance. First, let us set the seed, turn the 'sentiment' variable (the one we are trying to predict) into a factor (since it's currently a character vector), and split the data.

```{r}
#Building the Logistic Regression Model
#Setting the seed.
set.seed(69)

#turning sentiment into a factor so that we can actually predict it.
dataset_clean$sentiment <- as.factor(dataset_clean$sentiment)

#Reordering the factor so that 'Yes' is the first factor.
dataset_clean$sentiment <- relevel(dataset_clean$sentiment, 'positive')



#splitting the data.
dataset_split <- initial_split(dataset_clean, prop = 0.70,
                               strata = sentiment)

#splitting the data into a training set and a testing set.
dataset_train <- training(dataset_split)
dataset_test <- testing(dataset_split)
```

We can create the recipe using all of the predictors we added to our dataset above. Additionally, we will create the workflow and regression object at this time for our logistic regressoin model.

```{r}
#creating the recipe.
dataset_recipe <- recipe(sentiment ~ length + afinn_score + bing_score
                         + loughran_score + nrc_score, data = dataset_train)


#creating the logistic regression object.
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

#creating the workflow.
log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(dataset_recipe)
```

Finally, we can fit the model on the training data, then evaluate the performance (accuracy) of the model on the testing data as follows. We print out the accuracy at the end.

```{r}
#fitting the workflow with the object.
log_fit <- fit(log_wkflow, dataset_train)

#storing the accuracy of the logistic model on the testing data.
log_reg_acc <- augment(log_fit, new_data = dataset_test) %>%
  accuracy(truth = sentiment, estimate = .pred_class)
```

### Conclusion

Let's check the results of our model from above.

```{r}
log_reg_acc
```

We managed to obtain 100 percent accuracy with the model, which seems to indicate that there might be an issue with the model fit. When we ran the code, we were thrown a warning, which stated that the model fit was rank deficit. Changing the recipe in the code above reveals that we are thrown this error when 'loughran_score' is combined with at least one of the other score predictors. Excluding this predictor, we see that any combination of the other predictors always returns a 100 percent accuracy on the testing set, with the exception of the three following recipe combinations: 'sentiment \~ length', 'sentiment \~ length + affin_score', and 'sentiment \~ afinn_score'. (These recipe combinations yield accuracies of 50, 75, and 75 percent respectively on the testing dataset).

These results seem to indicate that the predictors from the sentiment engines we obtained are very good predictors for predicting the sentiments of movie reviews. It seems that all of them perform equivalently well, with the exception of 'AFINN', which, by our discussion above, does not perform as well.

In terms of problems with our sentiment analysis and what we could potentially fix, there are several things that are worth mentioning. First, even though lexicons attempt to cover all formats of certain words, such as how the AFINN lexicon included "abandon", "abandoned", and "abandons" at the same time, they fail to distinguish the different formats of some words that are not always used as verbs. For instance, in the AFINN lexicon, "lack" is included but "lacks" is not. Similarly, slang, abbreviations, and misspellings will also not be evaluated correctly, which can lead to less accurate sentiment assignments for some reviews.

To fix this, we can attempt the approach of approximate string comparisons, so that we can rate "lacks" the same way we rate "lack", and "terible" the same way we rate "terrible".

Another problem that we cannot ignore when using lexicon is the context of the reviews. For example, the word "scary" is rated negative in all the lexicons that contains it, and while it is usually a negative word, when the review is regarding a horror movie, it can actually be positive.

For future applications, we could solve this problem by including a new variable that accounts for words such as names of genres, which we will denote "multiplier". For instance, we can make "horror" a multiplier with a value of -1, and when "horror" is present, things like "scary" or "scream" are multiplied by this value, which will turn them into positive sentiments.
